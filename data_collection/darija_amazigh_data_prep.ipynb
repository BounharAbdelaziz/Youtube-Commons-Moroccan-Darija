{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import (\n",
    "    load_dataset,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    concatenate_datasets,\n",
    "    Audio,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_1 = \"BounharAbdelaziz/Moroccan-Darija-STT-Dataset\"\n",
    "DATA_PATH_2 = \"BounharAbdelaziz/Casablanca_cleaned\"\n",
    "DATA_PATH_3 = \"BounharAbdelaziz/Dvoice_cleaned\"\n",
    "DATA_PATH_4 = \"BounharAbdelaziz/Amazigh_ASR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUSH_DATA_TO = \"BounharAbdelaziz/Morocco-Darija-and-Amazigh-ASR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moroccan Arabic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_adrien = load_dataset(DATA_PATH_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_casa = load_dataset(DATA_PATH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dvoice = load_dataset(DATA_PATH_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_amazigh = load_dataset(DATA_PATH_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_adrien[\"train\"] = dataset_adrien[\"train\"].add_column('language', [\"moroccan_darija\"] * len(dataset_adrien[\"train\"]))\n",
    "dataset_adrien[\"validation\"] = dataset_adrien[\"validation\"].add_column('language', [\"moroccan_darija\"] * len(dataset_adrien[\"validation\"]))\n",
    "dataset_casa[\"train\"] = dataset_casa[\"train\"].add_column('language', [\"moroccan_darija\"] * len(dataset_casa[\"train\"]))\n",
    "dataset_dvoice[\"train\"] = dataset_dvoice[\"train\"].add_column('language', [\"moroccan_darija\"] * len(dataset_dvoice[\"train\"]))\n",
    "dataset_amazigh[\"train\"] = dataset_amazigh[\"train\"].add_column('language', [\"amazigh\"] * len(dataset_amazigh[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'a1pVO40Tzkw_segment_86.mp3',\n",
       "  'array': array([-0.0167015 ,  0.00307631,  0.0022051 , ..., -0.00553913,\n",
       "         -0.00444421, -0.0047589 ]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'شي حاجة فمشات قبل ما تجي عندي انا يعني مشات عند مجموعة ديال الرقاة فالإنسان بالسبب الإنسان يتخذ بالسبب من اجل العلاج فمني جات فعلا صرعت كاينة الحالة اللي كتعالج على الحصة وحدة في الحصة ديال التشخيص كتجي كتبغي تشخص الحالة ديالها تعرف شنو عندها فهاديك الحصة كييسر ليها الله تبارك وتعالى في العلاج. كاينين ثلاثة الحصات. كاينين سبعة الحصات على حسب النوع',\n",
       " 'language': 'moroccan_darija'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_adrien[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_adrien[\"train\"] = dataset_adrien[\"train\"].add_column('dataset_source', [\"adiren7/darija_speech_to_text\"] * len(dataset_adrien[\"train\"]))\n",
    "dataset_adrien[\"validation\"] = dataset_adrien[\"validation\"].add_column('dataset_source', [\"adiren7/darija_speech_to_text\"] * len(dataset_adrien[\"validation\"]))\n",
    "\n",
    "dataset_casa[\"train\"] = dataset_casa[\"train\"].add_column('dataset_source', [\"UBC-NLP/Casablanca\"] * len(dataset_casa[\"train\"]))\n",
    "\n",
    "dataset_dvoice[\"train\"] = dataset_dvoice[\"train\"].add_column('dataset_source', [\"dvoice\"] * len(dataset_dvoice[\"train\"]))\n",
    "\n",
    "dataset_amazigh[\"train\"] = dataset_amazigh[\"train\"].add_column('dataset_source', [\"TifinLab/amazigh_moroccan_asr\"] * len(dataset_amazigh[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract only audio, text, language and source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_casa = ['transcription_darija_ltn', 'intent', 'sentiment', 'gender_label', 'lead_time', 'sample_rate', 'split', 'duration']\n",
    "columns_to_drop_dvoice = ['transcription_darija_ltn', 'intent', 'sentiment', 'gender_label', 'lead_time', 'sample_rate', 'split', 'duration']\n",
    "columns_to_drop_amazigh = ['transcription_darija_ltn', 'intent', 'sentiment', 'gender_label', 'lead_time', 'sample_rate', 'split', 'duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_casa[\"train\"] = dataset_casa[\"train\"].remove_columns(columns_to_drop_casa)\n",
    "dataset_dvoice[\"train\"] = dataset_dvoice[\"train\"].remove_columns(columns_to_drop_dvoice)\n",
    "dataset_amazigh[\"train\"] = dataset_amazigh[\"train\"].remove_columns(columns_to_drop_amazigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename column to transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_casa[\"train\"] = dataset_casa[\"train\"].rename_column('transcription_darija_ar', 'transcription')\n",
    "dataset_dvoice[\"train\"] = dataset_dvoice[\"train\"].rename_column('transcription_darija_ar', 'transcription')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast to 16khz if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None), 'transcription': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'dataset_source': Value(dtype='string', id=None)}\n",
      "{'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'transcription': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'dataset_source': Value(dtype='string', id=None)}\n",
      "{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None), 'transcription': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'dataset_source': Value(dtype='string', id=None)}\n",
      "{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None), 'transcription': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'dataset_source': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_adrien[\"train\"].features)\n",
    "print(dataset_dvoice[\"train\"].features)\n",
    "print(dataset_casa[\"train\"].features)\n",
    "print(dataset_amazigh[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dvoice[\"train\"] = dataset_dvoice[\"train\"].cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = concatenate_datasets([dataset_adrien[\"train\"], dataset_dvoice[\"train\"], dataset_casa[\"train\"], dataset_amazigh[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "    num_rows: 11831\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": dataset_adrien[\"validation\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2367/2367 [00:02<00:00, 841.35 examples/s]/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 24/24 [00:01<00:00, 18.23ba/s]\n",
      "Map: 100%|██████████| 2366/2366 [00:05<00:00, 412.91 examples/s] 19.96s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 24/24 [00:01<00:00, 13.08ba/s]\n",
      "Map: 100%|██████████| 2366/2366 [00:06<00:00, 388.73 examples/s] 22.06s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 24/24 [00:01<00:00, 14.42ba/s]\n",
      "Map: 100%|██████████| 2366/2366 [00:00<00:00, 4081.43 examples/s]20.62s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 24/24 [00:00<00:00, 136.82ba/s]\n",
      "Map: 100%|██████████| 2366/2366 [00:00<00:00, 6654.09 examples/s]14.01s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 24/24 [00:00<00:00, 242.17ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 5/5 [01:09<00:00, 13.99s/it]\n",
      "Map: 100%|██████████| 1055/1055 [00:00<00:00, 1553.25 examples/s]s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 18.71ba/s]\n",
      "Map: 100%|██████████| 1055/1055 [00:06<00:00, 171.04 examples/s]  7.49s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:02<00:00,  5.29ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:27<00:00, 13.78s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Morocco-Darija-and-Amazigh-ASR/commit/87c3fcff54546d23afe682729a374b6c5b15e001', commit_message='Grouped all moroccan arabic and amazigh STT data', commit_description='', oid='87c3fcff54546d23afe682729a374b6c5b15e001', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Morocco-Darija-and-Amazigh-ASR', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Morocco-Darija-and-Amazigh-ASR'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(PUSH_DATA_TO, commit_message=\"Grouped all moroccan arabic and amazigh STT data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep only Moroccan Darija first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"BounharAbdelaziz/Morocco-Darija-and-Amazigh-ASR\"\n",
    "PUSH_DATA_TO = \"BounharAbdelaziz/Morocco-Darija-ASR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 11831/11831 [00:08<00:00, 1356.82 examples/s]\n",
      "Generating validation split: 100%|██████████| 2110/2110 [00:03<00:00, 550.25 examples/s] \n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 11831\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 2110\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 11831/11831 [01:24<00:00, 140.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].filter(lambda row: row['language'] == 'moroccan_darija')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 7591\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 2110\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make val smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_in_val = len(dataset['validation'])\n",
    "n_samples_to_keep_in_val = 300\n",
    "n_samples_to_put_in_train = n_samples_in_val - n_samples_to_keep_in_val\n",
    "\n",
    "seed=1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample n_samples_to_put_in_train rows from the validation split\n",
    "sampled_validation = dataset['validation'].shuffle(seed=seed).select(range(n_samples_to_put_in_train))\n",
    "\n",
    "# Concatenate the sampled rows with the train split\n",
    "dataset['train'] = concatenate_datasets([dataset['train'], sampled_validation])\n",
    "\n",
    "# Remove the sampled rows from the validation split\n",
    "dataset['validation'] = dataset['validation'].shuffle(seed=seed).select(range(n_samples_to_put_in_train, n_samples_in_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 9401\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1881/1881 [00:01<00:00, 1842.03 examples/s]s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 19/19 [00:00<00:00, 19.99ba/s]\n",
      "Map: 100%|██████████| 1880/1880 [00:01<00:00, 1699.17 examples/s]22.24s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 19/19 [00:01<00:00, 18.67ba/s]\n",
      "Map: 100%|██████████| 1880/1880 [00:01<00:00, 1182.43 examples/s]17.85s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 19/19 [00:02<00:00,  7.37ba/s]\n",
      "Map: 100%|██████████| 1880/1880 [00:00<00:00, 5409.34 examples/s]22.43s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 19/19 [00:00<00:00, 87.94ba/s]\n",
      "Map: 100%|██████████| 1880/1880 [00:02<00:00, 705.41 examples/s] 16.24s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 19/19 [00:02<00:00,  7.14ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 5/5 [01:35<00:00, 19.15s/it]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 1102.46 examples/s]t/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  6.04ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Morocco-Darija-ASR/commit/10042237e5fda64a0b4d8eff4e39d746619910b8', commit_message='Grouped all moroccan arabic STT data only, made val smaller.', commit_description='', oid='10042237e5fda64a0b4d8eff4e39d746619910b8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Morocco-Darija-ASR', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Morocco-Darija-ASR'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(PUSH_DATA_TO, commit_message=\"Grouped all moroccan arabic STT data only, made val smaller.\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
