{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import (\n",
    "    load_dataset,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    concatenate_datasets,\n",
    "    Audio,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUSH_DATA_TO = \"BounharAbdelaziz/French-and-English-ASR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"ylacombe/english_dialects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en_train_female = load_dataset(DATA_PATH, split= \"train\", name=\"northern_female\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en_train_male = load_dataset(DATA_PATH, split= \"train\", name=\"northern_male\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['line_id', 'audio', 'text', 'speaker_id'],\n",
       "    num_rows: 750\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_en_train_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['line_id', 'audio', 'text', 'speaker_id'],\n",
       "    num_rows: 2097\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_en_train_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en = DatasetDict({\n",
    "    \"train\": concatenate_datasets([dataset_en_train_female, dataset_en_train_male])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en[\"train\"] = dataset_en[\"train\"].remove_columns(['line_id', 'speaker_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 2847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"AdrienB134/Emilia-dataset-french-with-gender\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fr = load_dataset(DATA_PATH, split=\"test\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__key__', '__url__', 'json', 'audio', 'text', 'speaker_id', 'gender'],\n",
       "    num_rows: 1088\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_fr = ['__key__', '__url__', 'json', 'speaker_id', 'gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fr = dataset_fr.remove_columns(columns_to_drop_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fr = DatasetDict({\n",
    "    \"train\": dataset_fr\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en[\"train\"] = dataset_en[\"train\"].add_column('language', [\"english\"] * len(dataset_en[\"train\"]))\n",
    "dataset_fr[\"train\"] = dataset_fr[\"train\"].add_column('language', [\"french\"] * len(dataset_fr[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dataset source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en[\"train\"] = dataset_en[\"train\"].add_column('dataset_source', [\"ylacombe/english_dialects/northern_male_female\"] * len(dataset_en[\"train\"]))\n",
    "dataset_fr[\"train\"] = dataset_fr[\"train\"].add_column('dataset_source', [\"AdrienB134/Emilia-dataset-french-with-gender\"] * len(dataset_fr[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast to 16khz if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'text': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'dataset_source': Value(dtype='string', id=None)}\n",
      "{'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'text': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'dataset_source': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_fr[\"train\"].features)\n",
    "print(dataset_en[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en[\"train\"] = dataset_en[\"train\"].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset_fr[\"train\"] = dataset_fr[\"train\"].cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = concatenate_datasets([dataset_en[\"train\"], dataset_fr[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'text', 'language', 'dataset_source'],\n",
       "    num_rows: 3935\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetDict({\n",
    "    \"train\": train_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename column to transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"train\"] = train_dataset[\"train\"].rename_column('text', 'transcription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 3935\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 984/984 [00:01<00:00, 591.45 examples/s]it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:02<00:00,  4.81ba/s]\n",
      "Map: 100%|██████████| 984/984 [00:04<00:00, 200.27 examples/s]1, 23.99s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:01<00:00,  5.18ba/s]\n",
      "Map: 100%|██████████| 984/984 [00:04<00:00, 209.39 examples/s]9, 19.64s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:01<00:00,  5.48ba/s]\n",
      "Map: 100%|██████████| 983/983 [00:00<00:00, 3184.86 examples/s], 17.93s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 96.63ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 4/4 [00:59<00:00, 14.91s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/French-and-English-ASR/commit/ca019d345a43afa99305fb1422fd35b3fa52f14d', commit_message='French and English ASR data', commit_description='', oid='ca019d345a43afa99305fb1422fd35b3fa52f14d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/French-and-English-ASR', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/French-and-English-ASR'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.push_to_hub(PUSH_DATA_TO, commit_message=\"French and English ASR data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix with moroccan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_MA = \"BounharAbdelaziz/Morocco-Darija-and-Amazigh-ASR\"\n",
    "PUSH_MIXED_DATA_TO = \"BounharAbdelaziz/Mixed-Morocco-Darija-Amazigh-English-and-French-ASR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 11831/11831 [00:08<00:00, 1346.38 examples/s]\n",
      "Generating validation split: 100%|██████████| 2110/2110 [00:04<00:00, 525.79 examples/s] \n"
     ]
    }
   ],
   "source": [
    "data_ma = load_dataset(DATA_MA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mixed = concatenate_datasets([data_ma['train'], train_dataset['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_dataset = DatasetDict({\n",
    "    \"train\": train_dataset_mixed,\n",
    "    \"validation\": data_ma[\"validation\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 15766\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 2110\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1752/1752 [00:01<00:00, 1610.02 examples/s]s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 18/18 [00:00<00:00, 20.57ba/s]\n",
      "Map: 100%|██████████| 1752/1752 [00:01<00:00, 1291.06 examples/s]17.00s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 18/18 [00:00<00:00, 19.65ba/s]\n",
      "Map: 100%|██████████| 1752/1752 [00:02<00:00, 603.79 examples/s] 17.74s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 18/18 [00:02<00:00,  8.37ba/s]\n",
      "Map: 100%|██████████| 1752/1752 [00:00<00:00, 2741.34 examples/s]19.52s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 18/18 [00:00<00:00, 41.48ba/s]\n",
      "Map: 100%|██████████| 1752/1752 [00:00<00:00, 6811.36 examples/s]14.51s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 18/18 [00:00<00:00, 152.03ba/s]\n",
      "Map: 100%|██████████| 1752/1752 [00:00<00:00, 9488.41 examples/s] 0.55s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 18/18 [00:00<00:00, 282.04ba/s]\n",
      "Map: 100%|██████████| 1752/1752 [00:00<00:00, 2386.51 examples/s] 7.78s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 18/18 [00:00<00:00, 21.50ba/s]\n",
      "Map: 100%|██████████| 1751/1751 [00:02<00:00, 799.79 examples/s]  7.75s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 18/18 [00:03<00:00,  5.55ba/s]\n",
      "Map: 100%|██████████| 1751/1751 [00:00<00:00, 1847.02 examples/s]12.87s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 18/18 [00:01<00:00, 14.00ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 9/9 [01:53<00:00, 12.59s/it]\n",
      "Map: 100%|██████████| 1055/1055 [00:00<00:00, 1822.77 examples/s]s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 22.24ba/s]\n",
      "Map: 100%|██████████| 1055/1055 [00:02<00:00, 477.74 examples/s]  7.89s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:01<00:00,  5.83ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:27<00:00, 13.59s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Mixed-Morocco-Darija-Amazigh-English-and-French-ASR/commit/21ed6b067692a768c2c7828a2a86cfb92b7c330c', commit_message='mixed all current ASR data for the moroccan, amazigh, english and french languages.', commit_description='', oid='21ed6b067692a768c2c7828a2a86cfb92b7c330c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Mixed-Morocco-Darija-Amazigh-English-and-French-ASR', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Mixed-Morocco-Darija-Amazigh-English-and-French-ASR'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_dataset.push_to_hub(PUSH_MIXED_DATA_TO, commit_message=\"mixed all current ASR data for the moroccan, amazigh, english and french languages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
