{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import (\n",
    "    load_dataset,\n",
    "    DatasetDict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DATA_HUB = \"UBC-NLP/Casablanca\"\n",
    "# dataset = load_dataset(EVAL_DATA_HUB, \"Morocco\", split='test')\n",
    "dataset = load_dataset(EVAL_DATA_HUB, \"Morocco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '01 - Al Sir Laqdim - Ep 2 - السر القديم الحلقة_1152.3346875000002_1166.8978124999999_14500_1.wav',\n",
       "  'array': array([0.00048828, 0.00064087, 0.00061035, ..., 0.00039673, 0.00091553,\n",
       "         0.00100708]),\n",
       "  'sampling_rate': 44100},\n",
       " 'seg_id': '14500_1',\n",
       " 'transcription': 'فراسك أماما كون مهدي بقا ساكن معانا فالدار كون شديت الباك شحال هادي من نهار مشا عند الجيلالي مابقيتش كانعرف نقرا حيت هو لي كان كايشرحليا كلشي',\n",
       " 'gender': 'F',\n",
       " 'duration': 7.207262946}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'seg_id', 'transcription', 'gender', 'duration'],\n",
       "    num_rows: 1045\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '01 - Al Sir Laqdim - Ep 2 - السر القديم الحلقة_1054.2403125_1061.4290624999999_14498_1.wav',\n",
       "  'array': array([-0.00105286, -0.00083923, -0.00073242, ...,  0.00296021,\n",
       "          0.00283813,  0.00265503]),\n",
       "  'sampling_rate': 44100},\n",
       " 'seg_id': '14498_1',\n",
       " 'transcription': 'واش من نيتك أمريم كانقوليك أو موان بقاي تعلمي مي سعاد قوليها راني غادا عند مهدي يعاوني في الخدمة',\n",
       " 'gender': 'M',\n",
       " 'duration': 5.623854258}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_duration(example):\n",
    "    \"\"\"\n",
    "    Compute the duration of an audio file in seconds.\n",
    "    \n",
    "    Parameters:\n",
    "        example (dict): A dictionary containing the 'audio' column \n",
    "                        with raw waveform data and sample rate.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated example with 'duration' field in seconds.\n",
    "    \"\"\"\n",
    "    waveform = example['audio']['array']\n",
    "    sample_rate = example['audio']['sampling_rate']\n",
    "    duration = len(waveform) / sample_rate\n",
    "    example['duration'] = duration\n",
    "    return example\n",
    "\n",
    "def filter_short_audio(dataset, max_duration=4):\n",
    "    \"\"\"\n",
    "    Filter dataset to keep only audio files with duration less than max_duration.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset (Dataset): A Hugging Face Dataset containing an 'duration' column.\n",
    "        max_duration (float): Maximum allowed duration in seconds.\n",
    "    \n",
    "    Returns:\n",
    "        Dataset: Filtered dataset with elements having duration < max_duration.\n",
    "    \"\"\"\n",
    "    # Add duration column\n",
    "    dataset = dataset.map(compute_duration)\n",
    "    \n",
    "    # Filter based on duration\n",
    "    filtered_dataset = dataset.filter(lambda example: example['duration'] < max_duration)\n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = load_dataset(\"BounharAbdelaziz/Morocco-Darija-ASR\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add duration column\n",
    "dataset = eval_ds.map(compute_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'transcription', 'language', 'dataset_source', 'duration'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = filter_short_audio(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = DatasetDict({\"validation\": filtered_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27ad5e2368a43168ef795411d3cf214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b171974cb474d2ebc245caf2ed6880e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358b191464ac4357b552ea25d5bf85ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/atlasia/Morocco-Youtube-Commons-Eval/commit/f84c4aad54927805d139b52117babdfc15818184', commit_message='Kept audios < 4s as these have better transcriptions.', commit_description='', oid='f84c4aad54927805d139b52117babdfc15818184', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/atlasia/Morocco-Youtube-Commons-Eval', endpoint='https://huggingface.co', repo_type='dataset', repo_id='atlasia/Morocco-Youtube-Commons-Eval'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.push_to_hub(\"atlasia/Morocco-Youtube-Commons-Eval\", commit_message=\"Kept audios < 4s as these have better transcriptions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix my training set, remove interesection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import (\n",
    "    load_dataset,\n",
    "    DatasetDict,\n",
    "    Dataset,\n",
    "    concatenate_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ds = load_dataset(\"BounharAbdelaziz/Morocco-Darija-ASR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = load_dataset(\"atlasia/Moroccan-Darija-Youtube-Commons-Eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 9401\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source', 'duration'],\n",
       "        num_rows: 105\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = concatenate_datasets([current_ds['train'], current_ds['validation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = DatasetDict({\"train\": new_ds, \"validation\": eval_set['validation']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 9701\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source', 'duration'],\n",
       "        num_rows: 105\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds['validation'] = new_ds['validation'].remove_columns(\"duration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we remove duplicates and also items in train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_datasets(dataset_dict):\n",
    "    \"\"\"\n",
    "    Efficiently remove duplicates within training set and elements that appear in validation set.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dict (DatasetDict): Input dataset containing 'train' and 'validation' splits\n",
    "        \n",
    "    Returns:\n",
    "        DatasetDict: Cleaned dataset with no duplicates in train and no overlap with validation\n",
    "    \"\"\"\n",
    "    # First remove duplicates within training set\n",
    "    seen_transcriptions = set()\n",
    "    \n",
    "    def is_unique(example):\n",
    "        if example['transcription'] in seen_transcriptions:\n",
    "            return False\n",
    "        seen_transcriptions.add(example['transcription'])\n",
    "        return True\n",
    "    \n",
    "    # Remove duplicates from training set\n",
    "    deduped_train = dataset_dict['train'].filter(\n",
    "        is_unique,\n",
    "        num_proc=1  # Must be 1 for stateful filtering\n",
    "    )\n",
    "    \n",
    "    # Then remove validation overlaps\n",
    "    val_transcriptions = set(dataset_dict['validation']['transcription'])\n",
    "    \n",
    "    clean_train = deduped_train.filter(\n",
    "        lambda x: x['transcription'] not in val_transcriptions,\n",
    "        num_proc= 16\n",
    "    )\n",
    "    \n",
    "    return DatasetDict({\n",
    "        'train': clean_train,\n",
    "        'validation': dataset_dict['validation']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 9701/9701 [03:01<00:00, 53.52 examples/s]\n",
      "Filter (num_proc=16): 100%|██████████| 7574/7574 [00:17<00:00, 430.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_dataset = deduplicate_datasets(new_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training samples: 9701\n",
      "Cleaned training samples: 7469\n",
      "Removed samples: 2232\n"
     ]
    }
   ],
   "source": [
    "# Print statistics\n",
    "print(f\"Original training samples: {len(new_ds['train'])}\")\n",
    "print(f\"Cleaned training samples: {len(cleaned_dataset['train'])}\")\n",
    "print(f\"Removed samples: {len(new_ds['train']) - len(cleaned_dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 7469\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'transcription', 'language', 'dataset_source'],\n",
       "        num_rows: 105\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1494/1494 [00:00<00:00, 1533.20 examples/s]s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 15/15 [00:01<00:00, 11.76ba/s]\n",
      "Map: 100%|██████████| 1494/1494 [00:00<00:00, 1519.32 examples/s]13.90s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 15/15 [00:01<00:00, 10.79ba/s]\n",
      "Map: 100%|██████████| 1494/1494 [00:01<00:00, 1277.31 examples/s]13.99s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 15/15 [00:01<00:00,  9.08ba/s]\n",
      "Map: 100%|██████████| 1494/1494 [00:01<00:00, 761.23 examples/s] 13.89s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 15/15 [00:02<00:00,  5.61ba/s]\n",
      "Map: 100%|██████████| 1493/1493 [00:00<00:00, 1658.73 examples/s]15.16s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 15/15 [00:01<00:00, 12.38ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 5/5 [01:09<00:00, 13.89s/it]\n",
      "Map: 100%|██████████| 105/105 [00:00<00:00, 12702.31 examples/s]/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 193.04ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Morocco-Darija-ASR-v1.2/commit/dab32da41ef60fe7b1df28cfbc3724a55dbc9c57', commit_message='Deduplicated training set.', commit_description='', oid='dab32da41ef60fe7b1df28cfbc3724a55dbc9c57', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Morocco-Darija-ASR-v1.2', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Morocco-Darija-ASR-v1.2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset.push_to_hub(\"BounharAbdelaziz/Morocco-Darija-ASR-v1.2\", commit_message=\"Deduplicated training set.\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
